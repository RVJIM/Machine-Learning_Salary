{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "table = pd.read_excel(\"Ask A Manager Salary Survey 2021 (Responses).xlsx\")\n",
    "df = pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete na values of last 3 columns\n",
    "df.dropna(subset=df.iloc[:, -3:].columns, inplace=True)\n",
    "df.dropna(subset=df.columns[2], inplace=True)\n",
    "df.iloc[:, 3] = df.iloc[:, 3].replace('na', np.nan)\n",
    "df.dropna(subset=df.columns[3], inplace=True)\n",
    "\n",
    "# Delete columns that we don't use\n",
    "columns_to_delete = [\n",
    "                'Timestamp', 'If your job title needs additional context, please clarify here:',\n",
    "                'If your income needs additional context, please provide it here:', \n",
    "                \"If you're in the U.S., what state do you work in?\", 'What city do you work in?', \n",
    "                'How many years of professional work experience do you have overall?'\n",
    "                ]\n",
    "\n",
    "df.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "# Change columns' name\n",
    "names_columns = ['Age', 'Industry', 'Job Title', 'Salary', 'Additional', 'Currency', 'Other Currency', 'Country', 'Years job',\n",
    "                 'Highest Education', 'Gender', 'Race']\n",
    "df.columns = names_columns\n",
    "\n",
    "# Delete elements where the salary is equal to 0 \n",
    "df = df[df.loc[:, 'Salary'] != 0]\n",
    "\n",
    "df['Country'] = df['Country'].str.strip()\n",
    "df['Other Currency'] = df['Other Currency'].str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered by USA\n",
    "us = [\n",
    "        'U.S', 'U.S>', 'United States', 'usa', 'United States of America', 'United States Of America', 'UXZ',\n",
    "        'U.S.A.', 'US', 'Usa', 'The United State', 'Usa ', 'UsA', 'United  States', 'U.S.A', 'USaa', 'Unted States',\n",
    "        'United statew', 'United Sttes', 'Unitied States', 'USAB', 'Uniited States', 'Untied States', 'United Statues',\n",
    "        'United Statesp', 'Uniteed States', 'USS', 'U.s.a.', 'U.SA', 'united stated', 'United Stattes', 'United Statees',\n",
    "        'UNited States', 'Uniyed states', 'Uniyed states', 'Uniyed States', 'United States of Americas', 'US of A', 'UA',\n",
    "        'United Statss', 'uS', 'USD', 'United states of america', 'United States is America', 'america', 'United Statws',\n",
    "        'United Stateds', 'U. S', 'Uniter Statez', 'united states of aamerica', 'uSA', 'USaa', 'United STates', 'Unitef Stated',\n",
    "        'Usat', 'San Francisco', 'United States of American', 'United Status', 'United Sates of America', 'u.s.', 'United y',\n",
    "        'Unite States', 'The US', 'USA tomorrow', 'IS', 'is', 'I.S.', 'United Statea', 'ISA','U.s.', 'United Stares',\n",
    "        'Unites states', 'United State of America', 'ðŸ‡ºðŸ‡¸', 'united States', 'UnitedStates', 'United states of America',\n",
    "        'United Sates', 'The United States', 'UNITED STATES', 'United States of america', 'Unites States', 'United State',\n",
    "        'America', 'Us', 'united states', 'United states', 'USA', 'US', 'U.S.', 'California', 'america', 'Hartford', 'U.A.', \n",
    "        'USA-- Virgin Islands', 'united states of america', 'Uniyes States', 'U. S.', 'us', 'United Stated', 'Virginia',\n",
    "        'Worldwide (based in US but short term trips aroudn the world', 'USA (company is based in a US territory, I work remote)',\n",
    "        'United States (I work from home and my clients are all over the US/Canada/PR', 'For the United States government, but posted overseas',\n",
    "        'Currently finance', 'bonus based on meeting yearly goals set w/ my supervisor', 'Y', 'US govt employee overseas, country withheld',\n",
    "        \"I earn commission on sales. If I meet quota, I'm guaranteed another 16k min. Last year i earned an additional 27k. It's not uncommon for people in my space to earn 100k+ after commission.\",\n",
    "        'I work for a UAE-based organization, though I am personally in the US.', \"USA, but for foreign gov't\", 'Remote'\n",
    "        ]\n",
    "\n",
    "# Filtered by Canada\n",
    "canada = [\n",
    "        'Canada', 'canada', 'CANADA', 'Canda', 'Canadw', 'Can', 'Canad', 'CanadÃ¡', 'Csnada', 'Canada, Ottawa, ontario',\n",
    "        'I am located in Canada but I work for a company in the US', '$2,17.84/year is deducted for benefits', 'Policy'\n",
    "        ]\n",
    "\n",
    "# Filtered by UK\n",
    "uk = [\n",
    "        'United Kingdom', 'UK', 'England', 'Uk', 'Scotland', 'United kingdom', 'U.K.', 'united kingdom', 'uk',\n",
    "        'Great Britain', 'England, UK', 'Wales', 'England, United Kingdom', 'Northern Ireland', 'Scotland, UK',\n",
    "        'england', 'UK (England)', 'United Kingdom (England)', 'Wales (UK)', 'Wales, UK', 'Northern Ireland, United Kingdom',\n",
    "        'London', 'ENGLAND', 'Unites kingdom', 'England/UK', 'United Kingdom.', 'UK (Northern Ireland)', 'United Kindom',\n",
    "        'United Kingdomk', 'UK, remote', 'England, UK.', 'Britain', 'Englang', 'Wales (United Kingdom)', 'U.K',\n",
    "        'England, Gb', 'U.K. (northern England)', 'Isle of Man', 'UK for U.S. company'\n",
    "        ]\n",
    "\n",
    "df['Country'].replace(us, 'USA', inplace=True)\n",
    "df['Country'].replace(canada, 'Canada', inplace=True)\n",
    "df['Country'].replace(uk, 'United Kingdom', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errage = {\n",
    "        'INDIA': 'India', 'Sri lanka': 'Sri Lanka', 'pakistan':'Pakistan', 'ARGENTINA BUT MY ORG IS IN THAILAND': 'Argentina',\n",
    "        'United States- Puerto Rico': 'Puerto Rico', 'MÃ©xico':'Mexico', 'Brasil': 'Brazil', 'NZ': 'New Zealand',\n",
    "        'New zealand': 'New Zealand', 'australia': 'Australia', 'Australian': 'Australia', 'New Zealand Aotearoa': 'New Zealand',\n",
    "        'Australi': 'Australia', 'Aotearoa New Zealand': 'New Zealand', 'new zealand': 'New Zealand',  'japan': 'Japan',\n",
    "        'From New Zealand but on projects across APAC': 'New Zealand', 'Remote (philippines)': 'Philippines', 'FRANCE': 'France',\n",
    "        'singapore': 'Singapore', 'UAE': 'United Arab Emirates', 'Japan, US Gov position': 'Japan','Mainland China': 'China',\n",
    "        'hong konh': 'Hong Kong', 'NIGERIA': 'Nigeria', 'South africa': 'South Africa', 'europe': 'Czech Republic',\n",
    "        'the Netherlands': 'The Netherlands', 'NL': 'The Netherlands', 'Danmark': 'Denmark', 'Czechia': 'Czech Republic',\n",
    "        'Czech republic': 'Czech Republic', 'Company in Germany. I work from Pakistan.': 'Germany', 'croatia': 'Croatia',\n",
    "        'finland':'Finland', 'france': 'France', 'czech republic': 'Czech Republic', 'denmark': 'Denmark', 'spain': 'Spain',\n",
    "        'From Romania, but for an US based company': 'Romania', 'Austria, but I work remotely for a Dutch/British company': 'Austria',\n",
    "        'SWITZERLAND': 'Switzerland', 'switzerland': 'Switzerland', 'Luxemburg': 'Luxembourg', 'The netherlands': 'The Netherlands',\n",
    "        'ireland': 'Ireland', 'netherlands': 'The Netherlands', 'germany': 'Germany', 'Netherlands': 'The Netherlands',\n",
    "        'Nederland': 'Netherlands', 'Nederland': 'The Netherlands', 'Italy (South)': 'Italy', 'Catalonia': 'Spain',\n",
    "        'Jersey, Channel islands': 'Jersey', 'the netherlands': 'The Netherlands'\n",
    "        }\n",
    "\n",
    "df['Country'].replace(errage, inplace=True)\n",
    "\n",
    "countries = [\n",
    "        'Afghanistan', 'Argentina', 'Australia', 'Austria', 'Bangladesh', 'Belgium', 'Bermuda', 'Bosnia and Herzegovina', 'Brazil',\n",
    "        'Bulgaria', 'Cambodia', 'Canada', 'Cayman Islands', 'Chile', 'China', 'Colombia', 'Congo', 'Costa Rica', \"Cote d'Ivoire\", 'Croatia',\n",
    "        'Cuba', 'Cyprus', 'Czech Republic', 'Denmark', 'Ecuador', 'Eritrea', 'Estonia', 'Finland', 'France', 'Germany', 'Ghana', 'Greece',\n",
    "        'Hong Kong', 'Hungary', 'India', 'Indonesia', 'Ireland', 'Israel', 'Italy', 'Jamaica', 'Japan', 'Jersey', 'Jordan', 'Kenya',\n",
    "        'Kuwait', 'Latvia', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Malaysia', 'Malta', 'Mexico', 'Morocco', 'New Zealand', 'Nigeria',\n",
    "        'Norway', 'Pakistan', 'PanamÃ¡', 'Philippines', 'Poland', 'Portugal', 'Puerto Rico', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Saudi Arabia',\n",
    "        'Serbia', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Somalia', 'South Africa', 'South Korea', 'Spain', 'Sri Lanka', 'Sweden',\n",
    "        'Switzerland', 'Taiwan', 'Thailand', 'The Bahamas', 'The Netherlands', 'Trinidad and Tobago', 'Turkey', 'USA', 'Uganda', 'Ukraine',\n",
    "        'United Arab Emirates', 'United Kingdom', 'Uruguay', 'Vietnam'\n",
    "]\n",
    "\n",
    "df = df[df['Country'].isin(countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift value from Other Currency to Currency, delete the first and make Currency's values in Upper\n",
    "df['Currency'] = df['Currency'].astype(str)\n",
    "df['Other Currency'] = df['Other Currency'].astype(str)\n",
    "\n",
    "df = df[df['Other Currency'].str.len() <= 25] \n",
    "\n",
    "currencies = {\n",
    "        'US Dollar': 'USD', 'BRL (R$)': 'BRL', 'RM': 'MYR', 'Canadian': 'CAD', 'croatian kuna': 'HRK', 'Euro': 'EUR', 'Polish ZÅ‚oty': 'PLN',\n",
    "        'American Dollars': 'USD', 'AUD Australian': 'AUD', 'Australian Dollars': 'AUD', 'Singapore Dollara': 'SGD', 'canadian': 'CAD',\n",
    "        'RMB (chinese yuan)': 'CNY', 'Canadian': 'CAD', 'PLN (Polish zloty)': 'PLN', 'PLN (Zwoty)': 'PLN', 'NIS (new Israeli shekel)': 'ILS',\n",
    "        'Mexican Pesos': 'MXN', 'ILS/NIS': 'ILS', 'China RMB': 'CNY', 'Danish Kroner': 'DKK', 'czech crowns': 'CZK', 'Mexican pesos': 'MXN',\n",
    "        'Philippine peso (PHP)': 'PHP', 'PhP (Philippine Peso)': 'PHP', 'Israeli Shekels': 'ILS', 'Norwegian kroner (NOK)': 'NOK',\n",
    "        'Thai Baht': 'THB', 'Philippine Pesos': 'PHP', 'Taiwanese dollars': 'TWD', 'Argentine Peso': 'ARS', 'THAI  BAHT': 'THB', 'Rs': 'INR',\n",
    "        'Argentinian peso (ARS)': 'ARS', 'NTD': 'TWD', 'Peso Argentino': 'ARS', 'Philippine Peso': 'PHP', 'INR (Indian Rupee)': 'INR',\n",
    "        'Rupees': 'INR', 'Indian rupees': 'INR', 'KRW (Korean Won)': 'KRW', 'Korean Won': 'KRW'\n",
    "}\n",
    "\n",
    "df['Other Currency'].replace(currencies, inplace=True)\n",
    "df['Currency'].mask(df['Currency']=='Other', df['Other Currency'], inplace=True)\n",
    "df['Currency'].mask((df['Other Currency'].str.len() == 3) & (df['Other Currency']!= 'nan'), df['Other Currency'], inplace=True)\n",
    "df['Currency'] = df['Currency'].str.upper()\n",
    "\n",
    "df.drop(columns= 'Other Currency', inplace=True)\n",
    "elements_to_delete = ['NAN', 'N/A', 'EQUITY']\n",
    "df.drop(df[df['Currency'].isin(elements_to_delete)].index, inplace=True)\n",
    "\n",
    "df.loc[(df['Country'] == 'Australia') & (df['Currency'] == 'AUD/NZD'), 'Currency'] = 'AUD'\n",
    "df.loc[(df['Country'] == 'New Zealand') & (df['Currency'] == 'AUD/NZD'), 'Currency'] = 'NZD'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum of columns Salary and Additional\n",
    "df['Additional'].fillna(0, inplace=True) # otherwise, it creates a lot of blanks, because additional has them\n",
    "df['Salary'] = df['Salary'] + df['Additional']\n",
    "df.drop(columns='Additional', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the Salary in USD, every pair at 31/12/2021 and drop Currency column\n",
    "pair_currencies = {\n",
    "        'GBP': 1.3529, 'EUR': 1.1368, 'CAD': 0.7915, 'TRY': 0.07507, 'BRL': 0.1795, 'BR$': 0.1795, 'PHP': 0.01961, 'AUD': 0.7262,\n",
    "        'NZD': 0.6828, 'KWD': 3.3102, 'NGN': 0.00243, 'JPY': 0.00869, 'SEK': 0.1106, 'ZAR': 0.0625, 'PKR': 0.00569, 'MYR': 0.24015,\n",
    "        'PLN': 0.2480, 'SGD': 0.7413, 'HRK': 0.000915, 'CHF': 1.0962, 'TTD': 0.1476, 'CNY': 0.1574, 'SAR': 0.2664, 'ILS': 0.3221,\n",
    "        'MXN': 0.0488, 'CZK': 0.0458, 'DKK': 0.1529, 'HKD': 0.1283, 'THB': 0.0301, 'INR': 0.0134, 'NOK': 0.1135, 'TWD': 0.0317,\n",
    "        'ARS': 0.00974, 'LKR': 0.00493, 'KRW': 0.000842, 'COP': 0.000246, 'IDR': 0.0000702\n",
    "}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "        currency = row['Currency']\n",
    "        if currency in pair_currencies:\n",
    "                df.at[i, 'Salary'] *= pair_currencies[currency]\n",
    "                df.at[i, 'Currency'] = 'USD'\n",
    "\n",
    "df.drop(columns='Currency', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove few values not useful for our research\n",
    "df = df[df['Race'] != 'Another option not listed here or prefer not to answer']  \n",
    "df = df[(df['Gender'] != 'Other or prefer not to answer') & (df['Gender'] != 'Non-binary')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set threshold we use on Industry and Job Title columns to reduce elements \n",
    "threshold = 100\n",
    "\n",
    "df['Industry'] = df['Industry'].str.strip().str.capitalize()\n",
    "total_values_industry = df['Industry'].value_counts()\n",
    "df['Industry'] = np.where(df['Industry'].isin(total_values_industry[total_values_industry < threshold].index), 'OTHERS', df['Industry'])\n",
    "\n",
    "'''df['Job Title'] = df['Job Title'].str.strip().str.capitalize()\n",
    "total_values_job = df['Job Title'].value_counts()\n",
    "df['Job Title'] = np.where(df['Job Title'].isin(total_values_job[total_values_job < threshold].index), 'OTHERS', df['Job Title'])'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "job_titles = df['Job Title'].fillna('').values\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(job_titles)\n",
    "\n",
    "# k-means clustering\n",
    "num_clusters = 50  \n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Job Title Cluster'] = kmeans.labels_\n",
    "\n",
    "cluster_samples = {}\n",
    "for cluster_num in range(num_clusters):\n",
    "    cluster_samples[cluster_num] = df[df['Job Title Cluster'] == cluster_num]['Job Title'].head(20).values\n",
    "\n",
    "cluster_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_samples_text = []\n",
    "\n",
    "for cluster_num, samples in cluster_samples.items():\n",
    "    cluster_samples_text.append(f\"Cluster {cluster_num}:\\n\" + \"\\n\".join(map(str, samples)) + \"\\n\")\n",
    "\n",
    "cluster_samples_text = \"\\n\".join(cluster_samples_text)\n",
    "\n",
    "\n",
    "with open('cluster_samples.txt', 'w') as file:\n",
    "    file.write(cluster_samples_text)\n",
    "\n",
    "print('Cluster samples saved to cluster_samples.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('clean.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "# numerical columns\n",
    "for col in numerical_cols:\n",
    "    if col != 'Unnamed: 0':\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        ax = df[col].hist(color='skyblue', edgecolor='black', bins=20)\n",
    "        plt.title(f'Histogram of {col}', fontsize=16)\n",
    "        plt.xlabel(col, fontsize=14)\n",
    "        plt.ylabel('Frequency', fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        \n",
    "        for p in ax.patches:\n",
    "            ax.annotate(f\"{int(p.get_height())}\", (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='bottom', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# categorical columns\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if col == 'Country':  # show only top-10\n",
    "        top_countries = df[col].value_counts().nlargest(10)\n",
    "        top_countries.plot(kind='bar', color='lightcoral')\n",
    "    elif col == 'Race':  # show only top-10\n",
    "        top_race_answers = df[col].value_counts().nlargest(10)\n",
    "        top_race_answers.plot(kind='bar', color='lightcoral')\n",
    "    else:\n",
    "        df[col].value_counts().plot(kind='bar', color='lightcoral')\n",
    "    \n",
    "    plt.title(f'Distribution of {col}', fontsize=16)\n",
    "    plt.xlabel(col, fontsize=14)\n",
    "    plt.ylabel('Frequency', fontsize=14)\n",
    "    plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    \n",
    "    if col != 'Country' and col != 'Race':  \n",
    "        for i, v in enumerate(df[col].value_counts()):\n",
    "            plt.text(i, v + 0.2, str(v), ha='center', va='bottom', fontsize=10, color='black')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# statistics for numerical\n",
    "numerical_stats = df[numerical_cols].describe()\n",
    "print(\"Numerical Statistics:\")\n",
    "print(numerical_stats)\n",
    "\n",
    "# statistics for categorical\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nValue counts for {col}:\")\n",
    "    if col == 'Country':  \n",
    "        print(df[col].value_counts().nlargest(10))\n",
    "    elif col == 'Race':  \n",
    "        print(df[col].value_counts().nlargest(10))\n",
    "    else:\n",
    "        print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applicato molti preprocessing, ma Ã¨ meglio utilizzare solo OrdinalEncoder in quanto siamo davanti a colonne che sono solamente categoriali con piÃ¹ di 3 diversi valori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "\n",
    "df = df[(np.abs(stats.zscore(df['Salary'])) < 3)]\n",
    "df[\"Standard_sal\"] = MinMaxScaler().fit_transform(df[['Salary']])\n",
    "\n",
    "X = df.drop(['Salary', 'Job Title'], axis=1)\n",
    "y = df['Standard_sal']\n",
    "\n",
    "enc = OrdinalEncoder()\n",
    "X_transformed = enc.fit_transform(X)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_standardized = scaler.fit_transform(X_transformed)\n",
    "\n",
    "# Transform out polynomial features until 5 degrees\n",
    "number_degrees = [1,2,3,4]\n",
    "poly_x_values = []\n",
    "for degree in number_degrees:\n",
    "    poly_model = PolynomialFeatures(degree=degree)\n",
    "    poly_x_values.append(poly_model.fit_transform(X_standardized))\n",
    "\n",
    "# Split them in test and train\n",
    "X_trains, X_tests, y_trains, y_tests = [], [], [], []\n",
    "for x in poly_x_values:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=5)\n",
    "    X_trains.append(X_train)\n",
    "    X_tests.append(X_test)\n",
    "    y_trains.append(y_train)\n",
    "    y_tests.append(y_test)\n",
    "\n",
    "for x, y in zip(X_trains, y_trains):\n",
    "    print(x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Transform out polynomial features until 5 degrees\n",
    "number_degrees = [1,2,3]\n",
    "poly_x_values = []\n",
    "for degree in number_degrees:\n",
    "    poly_model = PolynomialFeatures(degree=degree)\n",
    "    poly_x_values.append(poly_model.fit_transform(X_train))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "from xgboost import XGBRegressor, to_graphviz\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def models(X_transformed, y, name_decision_tree, k=5):\n",
    "    df_models = pd.DataFrame()\n",
    "    df_models_val = pd.DataFrame()\n",
    "    \n",
    "    models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge': Ridge(),\n",
    "            'Lasso': Lasso(),\n",
    "            'RandomForestRegressor': RandomForestRegressor(),\n",
    "            'DecisionTreeRegressor': DecisionTreeRegressor(),\n",
    "            'XGBoost' : XGBRegressor(tree_method='hist')\n",
    "        }\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(model_name)\n",
    "        for x_train, x_test, y_train, y_test, degree in zip(X_train, X_test, Y_train, Y_test, number_degrees):\n",
    "            if model_name in ['Linear Regression', 'Ridge', 'Lasso']:\n",
    "                print(degree)\n",
    "                poly_model = PolynomialFeatures(degree=degree)\n",
    "                poly_model.fit_transform(X_transformed)\n",
    "\n",
    "            start_time = time.time()\n",
    "        \n",
    "            model.fit(x_train, y_train)\n",
    "            y_train_pred = model.predict(x)\n",
    "            #y_test_pred = model.predict(X_test)\n",
    "\n",
    "            train_mse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "            #test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "            r_squared_train = r2_score(y_train, y_train_pred)\n",
    "            #r_squared_test = r2_score(y_test, y_test_pred)\n",
    "        \n",
    "            row = {'Model': model_name,\n",
    "                'Run Time (minutes)': round((time.time() - start_time) / 60, 6),\n",
    "                'MSE': train_mse,\n",
    "                'R2': r_squared_train\n",
    "            }\n",
    "        \n",
    "            df_models = pd.concat([df_models, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Perform cross-validation\n",
    "            cv_scores_mse = -cross_val_score(model, x, y_train, cv=k, scoring='neg_mean_squared_error')\n",
    "            cv_scores_r2 = cross_val_score(model, x, y_train, cv=k, scoring='r2')\n",
    "\n",
    "            row_val = {'Model': model_name,\n",
    "                'Run Time (minutes)': round((time.time() - start_time) / 60, 6),\n",
    "                'Cross-Validated MSE': np.mean(cv_scores_mse),\n",
    "                'Cross-Validated R2': np.mean(cv_scores_r2)\n",
    "            }\n",
    "        \n",
    "            df_models_val = pd.concat([df_models_val, pd.DataFrame([row_val])], ignore_index=True)\n",
    "        \n",
    "            if model_name == 'DecisionTreeRegressor':\n",
    "                plt.figure(figsize=(20, 10))\n",
    "                plot_tree(model, filled=True, feature_names=X_train.columns if hasattr(X_train, 'columns') else None, rounded=True, max_depth=3)\n",
    "                os.makedirs('DecisioneTree', exist_ok=True)\n",
    "                plt.savefig(os.path.join('DecisioneTree', f'{name_decision_tree}.png'))\n",
    "                plt.close()\n",
    "\n",
    "            elif model_name == 'XGBoost':\n",
    "                graph_data = to_graphviz(model, num_trees=2, rankdir='LR')\n",
    "                graph_data.render(filename=os.path.join('XGBoost', f'{name_decision_tree}'), format='png', cleanup=True)\n",
    "\n",
    "            plt.scatter(degree, df_models.loc[degree-1, 'MSE'], color='blue')\n",
    "            plt.scatter(degree, df_models.loc[degree-1, 'R2'], color='gray')\n",
    "            \n",
    "        plt.plot(number_degrees, df_models[df_models['Model'] == model_name]['MSE'], color='red', label='MSE')\n",
    "        plt.plot(number_degrees, df_models[df_models['Model'] == model_name]['R2'], color='green', label='R2')\n",
    "        plt.xticks(range(1,len(number_degrees)+1))       \n",
    "        plt.grid(visible=None)\n",
    "        plt.legend()\n",
    "        plt.xlabel('Degree')\n",
    "        plt.ylabel('Values of Metrics')\n",
    "        plt.title(f'{name_decision_tree} - {model_name}')\n",
    "        os.makedirs('Accuracy', exist_ok=True)\n",
    "        plt.savefig(os.path.join('Accuracy', f'{model_name}.png'))\n",
    "        plt.close()\n",
    "        \n",
    "    return df_models, df_models_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_main, df_models_val_main = models(X_trains, X_tests, y_trains, y_tests, 'Main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_val_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis without Country, but with categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_sal = ['Luxembourg', 'Ireland', 'Singapore', 'Qatar', 'United Arab Emirates', 'Switzerland', 'USA', 'Norway', 'Denmark', 'The Netherlands', 'Iceland']\n",
    "mid_sal = ['Saudi Arabia', 'Austria', 'Sweden', 'Belgium', 'Germany', 'Australia', 'Finland', 'Canada', 'France', 'South Korea', 'UK', 'Italy', 'Israel', 'Japan',\n",
    "            'New Zealand', 'Slovenia', 'Kuwait', 'Spain']\n",
    "low_sal = ['Lithuania', 'Czech Republic', 'Poland', 'Portugal', 'Bahamas', 'Croatia', 'Hungary', 'Estonia', 'Panama', 'Slovakia', 'Turkey', 'Puerto Rico', 'Romania',\n",
    "            'Seychelles', 'Latvia', 'Greece']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories(x):\n",
    "    if x in high_sal: return 'High'\n",
    "    elif x in mid_sal: return 'Medium'\n",
    "    elif x in low_sal: return 'Low'\n",
    "    else: return 'Poverty'\n",
    "\n",
    "df_category = pd.DataFrame()\n",
    "df_category = df.copy()\n",
    "df_category['Category'] = df_category['Country'].apply(categories)\n",
    "df_category['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat = df_category.drop(['Standard_sal', 'Salary', 'Job Title'], axis=1)\n",
    "y = df['Standard_sal']\n",
    "\n",
    "X_transformed_cat = enc.fit_transform(X_cat)\n",
    "\n",
    "X_standardized_cat = scaler.fit_transform(X_transformed_cat)\n",
    "\n",
    "# Transform out polynomial features until 5 degrees\n",
    "poly_x_values_cat = []\n",
    "for degree in number_degrees:\n",
    "    poly_model = PolynomialFeatures(degree=degree)\n",
    "    poly_x_values_cat.append(poly_model.fit_transform(X_transformed_cat))\n",
    "\n",
    "# Split them in test and train\n",
    "X_trains_cat, X_tests_cat, y_trains_cat, y_tests_cat = [], [], [], []\n",
    "for x in poly_x_values_cat:\n",
    "    X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(x, y, train_size=0.8, random_state=5)    \n",
    "    X_trains_cat.append(X_train_cat)\n",
    "    X_tests_cat.append(X_test_cat)\n",
    "    y_trains_cat.append(y_train_cat)\n",
    "    y_tests_cat.append(y_test_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prova senza Job Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''X = df.drop(['Salary', 'Job Title'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=5)\n",
    "\n",
    "preprocessing = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(), ['Years job', 'Age', 'Highest Education']),\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore'), ['Category', 'Industry', 'Race'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train_transformed = preprocessing.fit_transform(X_train)\n",
    "X_test_transformed = preprocessing.transform(X_test)\n",
    "X_train_transformed.shape, X_test_transformed.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_transformed, df_models_transformed_val = models(X_trains_cat, X_trains_cat, y_trains_cat, y_tests_cat, 'Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_models_transformed_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_categories(df, category):\n",
    "    df_category = df[df['Category'] == category]\n",
    "    X = df_category.drop(['Salary'], axis=1)\n",
    "    y = df_category['Standard_sal']\n",
    "\n",
    "    X_transformed = enc.fit_transform(X)\n",
    "    \n",
    "    # Transform out polynomial features until 5 degrees\n",
    "    poly_x_values = []\n",
    "    for degree in number_degrees:\n",
    "        poly_model = PolynomialFeatures(degree=degree)\n",
    "        poly_x_values.append(poly_model.fit_transform(X_transformed))\n",
    "\n",
    "    # Split them in test and train\n",
    "    X_trains, X_tests, y_trains, y_tests = [], [], [], []\n",
    "    for x in poly_x_values:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(x, y, train_size=0.8, random_state=5)    \n",
    "        X_trains.append(X_train)\n",
    "        X_tests.append(X_test)\n",
    "        y_trains.append(y_train)\n",
    "        y_tests.append(y_test)\n",
    "    \n",
    "    df_models, df_models_val = models(X_trains, X_tests, y_trains, y_tests, category)\n",
    "    \n",
    "    return df_models, df_models_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to compare the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_subsets(df, categories):\n",
    "    all_results = []\n",
    "\n",
    "    for category in categories:\n",
    "        print(f\"Analyzing category: {category}\")\n",
    "        results, results_val = analysis_categories(df, category)\n",
    "        all_results.append(results)\n",
    "        \n",
    "    df_results = pd.concat(all_results, ignore_index=True)\n",
    "    return df_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_category['Category'].unique()\n",
    "df_results = compare_subsets(df_category, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.drop(columns='Run Time (minutes)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
